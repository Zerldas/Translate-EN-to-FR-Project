{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zerldas/Translate-EN-to-FR-Project/blob/main/NLP_Project_Translate_EN_to_FR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoirXTd-QsAf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 --version"
      ],
      "metadata": {
        "id": "-ieY7aXcsiZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxL-D2cDsHxW"
      },
      "source": [
        "**Kiểm tra và tải về các thư viện cần thiết trước khi thực hiện project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dxafpznvr1j-"
      },
      "outputs": [],
      "source": [
        "# Tải về các thư viện cần thiết trước khi thực hiện project\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q4KZHiAr6BI"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGiq5nnntg3L"
      },
      "source": [
        "***Danh sách các thư viện sẽ được sử dụng***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALlSiFXgtdiF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import spacy\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thiết lập GPU"
      ],
      "metadata": {
        "id": "yHNiP6RGVJOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", DEVICE)"
      ],
      "metadata": {
        "id": "buI83iwdVLQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thiết lại tokenizer"
      ],
      "metadata": {
        "id": "rZY1gY4l3ziw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_en = spacy.load(\"en_core_web_sm\")\n",
        "tokenize_fr = spacy.load(\"fr_core_news_sm\")"
      ],
      "metadata": {
        "id": "gxXJs2Pp3yco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_en(sentence):\n",
        "  return [token.text.lower()\n",
        "          for token in tokenize_en(sentence)\n",
        "          if token.text.strip()]\n",
        "\n",
        "def tokenizer_fr(sentence):\n",
        "  return [token.text.lower()\n",
        "          for token in tokenize_fr(sentence)\n",
        "          if token.text.strip()]"
      ],
      "metadata": {
        "id": "yEPi97SL2guO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry1qzXThwt1C"
      },
      "source": [
        "***Tiến hành phân tích dữ liệu***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpCGCoU-wsep"
      },
      "outputs": [],
      "source": [
        "# Khai báo dường đẫn dữ liệu train\n",
        "train_en_path = \"/content/drive/MyDrive/EN-FR/data/train/train.en\"\n",
        "train_fr_path = \"/content/drive/MyDrive/EN-FR/data/train/train.fr\"\n",
        "# Dữ liệu valiation\n",
        "val_en_path = \"/content/drive/MyDrive/EN-FR/data/val/val.en\"\n",
        "val_fr_path = \"/content/drive/MyDrive/EN-FR/data/val/val.fr\"\n",
        "# Dữ liệu test sử dụng tạp test 2016 flick\n",
        "test_en_path = \"/content/drive/MyDrive/EN-FR/data/test/test_2016_flickr.en\"\n",
        "test_fr_path = \"/content/drive/MyDrive/EN-FR/data/test/test_2016_flickr.fr\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(en_path, fr_path):\n",
        "    with open(en_path, encoding=\"utf-8\") as f_en:\n",
        "        en_lines = [line.strip() for line in f_en]\n",
        "\n",
        "    with open(fr_path, encoding=\"utf-8\") as f_fr:\n",
        "        fr_lines = [line.strip() for line in f_fr]\n",
        "\n",
        "    assert len(en_lines) == len(fr_lines), \"EN-FR line count mismatch!\"\n",
        "    return en_lines, fr_lines"
      ],
      "metadata": {
        "id": "YIOp7ohkKRQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBr_M4_ByMRM"
      },
      "outputs": [],
      "source": [
        "train_en_lines, train_fr_lines = load_data(train_en_path, train_fr_path)\n",
        "val_en_lines, val_fr_lines = load_data(val_en_path, val_fr_path)\n",
        "test_en_lines, test_fr_lines = load_data(test_en_path, test_fr_path)\n",
        "\n",
        "print(\"=== TẬP TRAIN ===\")\n",
        "print(f\"Tiếng Anh: {len(train_en_lines)}\")\n",
        "print(f\"Tiếng Pháp: {len(train_fr_lines)}\")\n",
        "print(f\"Tổng: {len(train_en_lines) + len(train_fr_lines)}\\n\")\n",
        "\n",
        "print(\"=== TẬP VALIDATION ===\")\n",
        "print(f\"Tiếng Anh: {len(val_en_lines)}\")\n",
        "print(f\"Tiếng Pháp: {len(val_fr_lines)}\")\n",
        "print(f\"Tổng: {len(val_en_lines) + len(val_fr_lines)}\\n\")\n",
        "\n",
        "print(\"=== TẬP TEST ===\")\n",
        "print(f\"Tiếng Anh: {len(test_en_lines)}\")\n",
        "print(f\"Tiếng Pháp: {len(test_fr_lines)}\")\n",
        "print(f\"Tổng: {len(test_en_lines) + len(test_fr_lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Xem xét độ dài câu (token)\n",
        "def get_lengths(sentences, tokenizer):\n",
        "    return [len(tokenizer(s)) for s in sentences]\n",
        "\n",
        "train_len_en = get_lengths(train_en_lines, tokenize_en)\n",
        "train_len_fr = get_lengths(train_fr_lines, tokenize_fr)\n",
        "\n",
        "print(train_len_en)\n",
        "print(train_len_fr)"
      ],
      "metadata": {
        "id": "XjDDg8yuv1aE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Biểu đồ xem xét độ dài câu\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(train_len_en, train_len_fr, s=8, alpha=0.4)\n",
        "plt.xlabel(\"EN Length\")\n",
        "plt.ylabel(\"FR Length\")\n",
        "plt.title(\"Sentence Length Correlation (EN vs FR)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o9O1bYi65HTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_text = \" \".join(train_en_lines)\n",
        "fr_text = \" \".join(train_fr_lines)\n",
        "\n",
        "wordcloud_en = WordCloud(width=1000, height=600, background_color='white').generate(en_text)\n",
        "wordcloud_fr = WordCloud(width=1000, height=600, background_color='white').generate(fr_text)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud_en, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"English Word Cloud\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.imshow(wordcloud_fr, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"French Word Cloud\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ej6Wh3Nw8Ql7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xây dựng vocabulary"
      ],
      "metadata": {
        "id": "3ZChJYFO8mGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index_token = [\n",
        "  \"<pad>\", #Padding giúp câu có cùng độ dài\n",
        "  \"<unk>\", #Unknow đánh dấu từ không có nghĩa\n",
        "  \"<sos>\", #Từ ở đầu câu\n",
        "  \"<eos>\"  #TỪ ở cuối câu\n",
        "]\n",
        "\n",
        "PAD_IDX = index_token.index(\"<pad>\")   # 0\n",
        "UNK_IDX = index_token.index(\"<unk>\")   # 1\n",
        "SOS_IDX = index_token.index(\"<sos>\")   # 2\n",
        "EOS_IDX = index_token.index(\"<eos>\")   # 3"
      ],
      "metadata": {
        "id": "P2fnMZGFDD4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences, tokenizer, max_size=10000000):\n",
        "  counter = Counter()\n",
        "\n",
        "  for sen in sentences:\n",
        "    tokens = tokenizer(sen)\n",
        "    counter.update(tokens)\n",
        "\n",
        "  most_common = counter.most_common(max_size - 4)\n",
        "  # Danh sách từ\n",
        "  idx_token = index_token + [word for word, _ in most_common]\n",
        "  # map token → id\n",
        "  token_to_index = {token: idx for idx, token in enumerate(idx_token)}\n",
        "\n",
        "  return token_to_index, index_token, counter"
      ],
      "metadata": {
        "id": "cmrM6qXn8qio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build vocab\n",
        "vocab_en, index_en, counter_en = build_vocab(train_en_lines, tokenize_en, max_size=10000)\n",
        "vocab_fr, index_fr, counter_fr = build_vocab(train_fr_lines, tokenize_fr, max_size=10000)\n",
        "\n",
        "print(\"EN vocab size:\", len(vocab_en))\n",
        "print(\"FR vocab size:\", len(vocab_fr))"
      ],
      "metadata": {
        "id": "ToptzvcX_i71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đưa vocab về dạng số\n",
        "def numericalize(sentence, tokenizer, vocab):\n",
        "  tokens = [\"<sos>\"] + tokenizer(sentence) + [\"<eos>\"]\n",
        "  return [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]"
      ],
      "metadata": {
        "id": "3y04QrdpSDyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Translate_Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, en_sentences, fr_sentences):\n",
        "    self.en_sentences = en_sentences\n",
        "    self.fr_sentences = fr_sentences\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.en_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    en_sentence = self.en_sentences[index]\n",
        "    fr_sentence = self.fr_sentences[index]\n",
        "\n",
        "    # chuyển sang dạng số\n",
        "    source_ids = numericalize(en_sentence, tokenizer_en, vocab_en)\n",
        "    target_ids = numericalize(fr_sentence, tokenizer_fr, vocab_fr)\n",
        "\n",
        "    # độ dài câu nguồn để pack_padded_sequence\n",
        "    length = len(source_ids)\n",
        "\n",
        "    return torch.tensor(source_ids), torch.tensor(target_ids), length"
      ],
      "metadata": {
        "id": "i7LtoU2pt_i8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  source_batch, target_batch, source_lengths = zip(*batch)\n",
        "  source_lengths = list(source_lengths)\n",
        "  sorted_indices = sorted(range(len(source_lengths)), key=lambda i: -source_lengths[i])\n",
        "\n",
        "  source_batch = [source_batch[i] for i in sorted_indices]\n",
        "  target_batch = [target_batch[i] for i in sorted_indices]\n",
        "  source_lengths = [source_lengths[i] for i in sorted_indices]\n",
        "\n",
        "  source_batch = pad_sequence(\n",
        "      source_batch, batch_first=True, padding_value=PAD_IDX\n",
        "  )\n",
        "\n",
        "  target_batch = pad_sequence(\n",
        "      target_batch, batch_first=True, padding_value=PAD_IDX\n",
        "  )\n",
        "\n",
        "  # convert source_lengths to tensor\n",
        "  source_lengths = torch.tensor(source_lengths, dtype=torch.long)\n",
        "\n",
        "  return source_batch, target_batch, source_lengths"
      ],
      "metadata": {
        "id": "VOsDp-IzSKsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo DataLoaders\n",
        "train = Translate_Dataset(train_en_lines, train_fr_lines)\n",
        "val = Translate_Dataset(val_en_lines, val_fr_lines)\n",
        "test = Translate_Dataset(test_en_lines, test_fr_lines)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = torch.utils.data.DataLoader(val, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=32 ,shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "7kxINE7CvkyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "v3B0LYmmXsLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "      embed_size,\n",
        "      hidden_size,\n",
        "      num_layers=num_layers,\n",
        "      dropout=dropout,\n",
        "      batch_first=True\n",
        "    )\n",
        "\n",
        "  def forward(self, src, lengths):\n",
        "    embedded = self.embedding(src)\n",
        "\n",
        "    packed = nn.utils.rnn.pack_padded_sequence(\n",
        "      embedded, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
        "    )\n",
        "\n",
        "    outputs, (hidden, cell) = self.lstm(packed)\n",
        "\n",
        "    return hidden, cell"
      ],
      "metadata": {
        "id": "1FxHd76GXuWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "gxAHGzWsUFsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, pad_idx):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(\n",
        "      vocab_size,\n",
        "      embed_size,\n",
        "      padding_idx=pad_idx\n",
        "    )\n",
        "\n",
        "    self.lstm = nn.LSTM(\n",
        "      embed_size,\n",
        "      hidden_size,\n",
        "      num_layers=num_layers,\n",
        "      dropout=dropout,\n",
        "      batch_first=True\n",
        "    )\n",
        "\n",
        "    self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "  def forward(self, token, hidden, cell):\n",
        "    token = token.unsqueeze(1)\n",
        "    embedded = self.embedding(token)\n",
        "    output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
        "    prediction = self.fc_out(output.squeeze(1))\n",
        "\n",
        "    return prediction, hidden, cell"
      ],
      "metadata": {
        "id": "27TvTeaTUFM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seq2Seq"
      ],
      "metadata": {
        "id": "BM4E6C05UTJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "  def __init__(self, encoder, decoder, pad_index):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.pad_index = pad_index\n",
        "\n",
        "  def forward(self, source, target, source_lengths, teacher_forcing_ratio=0.5):\n",
        "    batch_size, target_sequence_length = target.size()\n",
        "    vocabulary_size = self.decoder.fc_out.out_features\n",
        "    device = source.device\n",
        "\n",
        "    # output_tensor = (batch_size, target_len, vocab_size)\n",
        "    outputs = torch.zeros(batch_size, target_sequence_length, vocabulary_size, device=device)\n",
        "\n",
        "    # Encoder returns: final_hidden_state, final_cell_state\n",
        "    hidden_state, cell_state = self.encoder(source, source_lengths)\n",
        "\n",
        "    # First input to decoder is always <sos>\n",
        "    decoder_input = target[:, 0]\n",
        "\n",
        "    for timestep in range(1, target_sequence_length):\n",
        "\n",
        "      decoder_output, hidden_state, cell_state = self.decoder(\n",
        "          decoder_input, hidden_state, cell_state\n",
        "      )\n",
        "\n",
        "      outputs[:, timestep] = decoder_output\n",
        "\n",
        "      use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "      predicted_token = decoder_output.argmax(1)\n",
        "\n",
        "      decoder_input = target[:, timestep] if use_teacher_forcing else predicted_token\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "oPhUgud-URj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model"
      ],
      "metadata": {
        "id": "75lPsSjkUpOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khai báo thông số model\n",
        "INPUT_SIZE = len(vocab_en)\n",
        "OUTPUT_SIZE = len(vocab_fr)\n",
        "ENC_EMB_SIZE = 256\n",
        "DEC_EMB_SIZE = 256\n",
        "HID_SIZE = 512\n",
        "NUM_LAYERS = 2\n",
        "DROPOUT = 0.5\n",
        "BATCH_SIZE = 32\n",
        "N_EPOCHS = 10\n",
        "LEARNING_RATE = 0.001"
      ],
      "metadata": {
        "id": "XwHGDKBpXyqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Thiết lập Early Stopping\n",
        "class EarlyStopping:\n",
        "  def __init__(self, patience=3, min_delta=0):\n",
        "    self.patience = patience\n",
        "    self.min_delta = min_delta\n",
        "    self.best_loss = float('inf')\n",
        "    self.counter = 0\n",
        "    self.should_stop = False\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    # Nếu cải thiện\n",
        "    if val_loss < self.best_loss - self.min_delta:\n",
        "      self.best_loss = val_loss\n",
        "      self.counter = 0\n",
        "    else:\n",
        "      self.counter += 1\n",
        "      print(f\"No improvement ({self.counter}/{self.patience})\")\n",
        "\n",
        "      if self.counter >= self.patience:\n",
        "        self.should_stop = True\n",
        "        print(\"\\n Early Stopping Triggered!\")"
      ],
      "metadata": {
        "id": "lGJtr0wofqKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_SIZE, ENC_EMB_SIZE, HID_SIZE, NUM_LAYERS, DROPOUT).to(DEVICE)\n",
        "decoder = Decoder(OUTPUT_SIZE, DEC_EMB_SIZE, HID_SIZE, NUM_LAYERS, DROPOUT, PAD_IDX).to(DEVICE)\n",
        "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "  optimizer,\n",
        "  mode='min',\n",
        "  factor=0.5,\n",
        "  patience=2,\n",
        "  threshold=1e-4,\n",
        "  cooldown=0,\n",
        "  min_lr=1e-6,\n",
        ")"
      ],
      "metadata": {
        "id": "tONhol8JUXjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "L7qTi6p1w9m5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "\n",
        "def train_epoch():\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  for source, target, source_lengths in train_loader:\n",
        "    source = source.to(DEVICE)\n",
        "    target = target.to(DEVICE)\n",
        "    source_lengths = source_lengths.to(DEVICE)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(source, target, source_lengths)\n",
        "\n",
        "    # Bỏ <sos> trong cả output và target\n",
        "    output = output[:, 1:].contiguous().view(-1, output.size(-1))\n",
        "    target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "y6DjzBISw9FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính Valid\n",
        "def eval_epoch():\n",
        "  model.eval()\n",
        "  total_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for source, target, source_lengths in val_loader:\n",
        "      source = source.to(DEVICE)\n",
        "      target = target.to(DEVICE)\n",
        "      source_lengths = source_lengths.to(DEVICE)\n",
        "\n",
        "      output = model(\n",
        "        source,\n",
        "        target,\n",
        "        source_lengths,\n",
        "        teacher_forcing_ratio=0.0\n",
        "      )\n",
        "\n",
        "      output = output[:, 1:].contiguous().view(-1, output.size(-1))\n",
        "      target = target[:, 1:].contiguous().view(-1)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  return total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "ZLSvoNJDxJtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "  train_loss = train_epoch()\n",
        "  val_loss = eval_epoch()\n",
        "\n",
        "  train_losses.append(train_loss)\n",
        "  val_losses.append(val_loss)\n",
        "\n",
        "  print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "  print(f\"Train Loss: {train_loss:.4f}\")\n",
        "  print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "  # Early stopping kiểm tra val_loss\n",
        "  early_stopping(val_loss)\n",
        "  if early_stopping.should_stop:\n",
        "    print(f\"\\n EARLY STOPPING IN {epoch}\")\n",
        "    break\n",
        "\n",
        "# Lưu mô hình\n",
        "torch.save(model.state_dict(), \"best_model.pth\")"
      ],
      "metadata": {
        "id": "p8h69cG3xfAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(train_losses, label=\"Train Loss\")\n",
        "  plt.plot(val_losses, label=\"Validation Loss\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  plt.title(\"Training & Validation Loss\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "plot_losses(train_losses, val_losses)"
      ],
      "metadata": {
        "id": "dX8fFUrALGgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm translate\n",
        "def translate(sentence):\n",
        "  model.eval()\n",
        "  source_ids = numericalize(sentence, tokenizer_en, vocab_en)\n",
        "  source_tensor = torch.tensor(source_ids, dtype=torch.long).unsqueeze(0).to(DEVICE)\n",
        "  source_lengths = [source_tensor.size(1)]\n",
        "  idx_to_fr = list(vocab_fr.keys())\n",
        "\n",
        "  with torch.no_grad():\n",
        "    hidden, cell = model.encoder(source_tensor, source_lengths)\n",
        "    token = torch.tensor([SOS_IDX], dtype=torch.long, device=DEVICE)\n",
        "\n",
        "    result_tokens = []\n",
        "\n",
        "    for _ in range(50):\n",
        "      output, hidden, cell = model.decoder(token, hidden, cell)\n",
        "\n",
        "      next_token = output.argmax(1)\n",
        "      idx = next_token.item()\n",
        "\n",
        "      if idx == EOS_IDX:\n",
        "          break\n",
        "\n",
        "      result_tokens.append(idx_to_fr[idx])\n",
        "      token = next_token\n",
        "\n",
        "  return \" \".join(result_tokens)"
      ],
      "metadata": {
        "id": "nEO4OB8wxm2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tính BLEU\n",
        "def compute_bleu(n=100):\n",
        "  smoothie = SmoothingFunction().method4\n",
        "  scores = []\n",
        "\n",
        "  for i in range(n):\n",
        "    pred_sentence = translate(test_en_lines[i]).lower()\n",
        "    pred_tokens = pred_sentence.split()\n",
        "\n",
        "    ref_tokens = tokenizer_fr(test_fr_lines[i].lower())\n",
        "\n",
        "    bleu = sentence_bleu([ref_tokens], pred_tokens, smoothing_function=smoothie)\n",
        "    scores.append(bleu)\n",
        "\n",
        "  return sum(scores) / len(scores)\n",
        "\n",
        "print(\"BLEU:\", compute_bleu(100))"
      ],
      "metadata": {
        "id": "JpwN6TjbxrtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dịch thử năm câu\n",
        "for i in range(5):\n",
        "  print(\"EN :\", test_en_lines[i])\n",
        "  print(\"PRED:\", translate(test_en_lines[i]))\n",
        "  print(\"REF :\", test_fr_lines[i])\n",
        "  print()"
      ],
      "metadata": {
        "id": "M9RcR49r-3R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./result ./drive/MyDrive/EN-FR/"
      ],
      "metadata": {
        "id": "M5L0BmF5Ee0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phần Nâng Cao"
      ],
      "metadata": {
        "id": "bmQJXSDeQEN8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thêm Cơ Chế Attention"
      ],
      "metadata": {
        "id": "fZqrYKboQGW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "  def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "    super().__init__()\n",
        "    # enc_hid_dim: kích thước encoder output (nếu bidirectional thì = enc_hid * 2)\n",
        "    # dec_hid_dim: kích thước hidden decoder\n",
        "    self.W_encoder = nn.Linear(enc_hid_dim, dec_hid_dim)\n",
        "    self.W_decoder = nn.Linear(dec_hid_dim, dec_hid_dim)\n",
        "    self.v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
        "\n",
        "  def forward(self, decoder_hidden, encoder_outputs, mask=None):\n",
        "    decoder_hidden = decoder_hidden.unsqueeze(1)\n",
        "\n",
        "    energy = torch.tanh(\n",
        "        self.W_encoder(encoder_outputs) + self.W_decoder(decoder_hidden)\n",
        "    )\n",
        "\n",
        "    score = self.v(energy).squeeze(2)\n",
        "\n",
        "    if mask is not None:\n",
        "        score = score.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    attn_weights = F.softmax(score, dim=1)\n",
        "\n",
        "    return attn_weights"
      ],
      "metadata": {
        "id": "WFd_4sXARu8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauDecoder(nn.Module):\n",
        "  def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "    self.attention = BahdanauAttention(enc_hid_dim, dec_hid_dim)\n",
        "\n",
        "    self.rnn = nn.GRU(emb_dim + enc_hid_dim, dec_hid_dim, batch_first=True)\n",
        "\n",
        "    self.fc_out = nn.Linear(emb_dim + enc_hid_dim + dec_hid_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input_token, last_hidden, encoder_outputs, mask=None):\n",
        "    embedded = self.dropout(self.embedding(input_token)).unsqueeze(1)  # [batch,1,emb]\n",
        "\n",
        "    # Prepare decoder hidden: [batch, dec_hid_dim]\n",
        "    dec_hidden = last_hidden.squeeze(0)\n",
        "\n",
        "    # attention weights: [batch, src_len]\n",
        "    attn_weights = self.attention(dec_hidden, encoder_outputs, mask)\n",
        "\n",
        "    # context vector: [batch, enc_hid_dim]\n",
        "    context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
        "\n",
        "    # RNN input\n",
        "    rnn_input = torch.cat([embedded.squeeze(1), context], dim=1).unsqueeze(1)\n",
        "\n",
        "    # GRU\n",
        "    output, hidden = self.rnn(rnn_input, last_hidden)\n",
        "    output = output.squeeze(1)  # [batch, dec_hid]\n",
        "\n",
        "    # final projection\n",
        "    logits = self.fc_out(torch.cat([output, context, embedded.squeeze(1)], dim=1))\n",
        "\n",
        "    return logits, hidden, attn_weights"
      ],
      "metadata": {
        "id": "7dOBnRjsR8lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqWithAttention(nn.Module):\n",
        "  def __init__(self, encoder, decoder, device):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.device = device\n",
        "\n",
        "  def create_mask(self, source_tokens, padding_index):\n",
        "    return (source_tokens != padding_index).to(self.device)\n",
        "\n",
        "  def forward(\n",
        "    self,\n",
        "    source_tokens,\n",
        "    source_lengths,\n",
        "    target_tokens=None,\n",
        "    teacher_forcing_ratio=0.5,\n",
        "    padding_index=None\n",
        "  ):\n",
        "\n",
        "    batch_size = source_tokens.size(0)\n",
        "\n",
        "    # Nếu không truyền padding_index → mặc định là 0\n",
        "    if padding_index is None:\n",
        "      padding_index = 0\n",
        "\n",
        "    # --- ENCODER ---\n",
        "    encoder_outputs, encoder_hidden_states = self.encoder(\n",
        "      source_tokens, source_lengths\n",
        "    )\n",
        "\n",
        "    # Mask dùng trong attention\n",
        "    attention_mask = self.create_mask(source_tokens, padding_index)\n",
        "\n",
        "    # --- CHUẨN HÓA HIDDEN ĐỂ CHO VÀO DECODER ---\n",
        "    if isinstance(encoder_hidden_states, tuple):\n",
        "      # Trường hợp encoder là LSTM\n",
        "      encoder_hidden_tensor = encoder_hidden_states[0]\n",
        "    else:\n",
        "      encoder_hidden_tensor = encoder_hidden_states\n",
        "\n",
        "    # Nếu encoder là bidirectional\n",
        "    if encoder_hidden_tensor.size(0) >= 2 and encoder_hidden_tensor.size(0) % 2 == 0:\n",
        "      last_forward_state = encoder_hidden_tensor[-2, :, :]\n",
        "      last_backward_state = encoder_hidden_tensor[-1, :, :]\n",
        "\n",
        "      decoder_initial_hidden = torch.tanh(\n",
        "        torch.cat((last_forward_state, last_backward_state), dim=1)\n",
        "      ).unsqueeze(0)\n",
        "\n",
        "    else:\n",
        "      decoder_initial_hidden = encoder_hidden_tensor[-1, :, :].unsqueeze(0)\n",
        "\n",
        "    if target_tokens is not None:\n",
        "      target_length = target_tokens.size(1)\n",
        "    else:\n",
        "      target_length = 50  # độ dài tối đa khi suy luận\n",
        "\n",
        "    outputs = torch.zeros(\n",
        "        batch_size, target_length, self.decoder.output_dim\n",
        "    ).to(self.device)\n",
        "\n",
        "    # Token đầu vào đầu tiên cho decoder là token <sos>\n",
        "    if target_tokens is not None:\n",
        "        decoder_input_token = target_tokens[:, 0]\n",
        "    else:\n",
        "        sos_index = 3\n",
        "        decoder_input_token = torch.LongTensor(\n",
        "          [sos_index] * batch_size\n",
        "        ).to(self.device)\n",
        "\n",
        "    decoder_hidden = decoder_initial_hidden\n",
        "\n",
        "    for t in range(1, target_length):\n",
        "        output_logits, decoder_hidden, attention_weights = self.decoder(\n",
        "          decoder_input_token,\n",
        "          decoder_hidden,\n",
        "          encoder_outputs,\n",
        "          mask=attention_mask\n",
        "        )\n",
        "\n",
        "        outputs[:, t, :] = output_logits\n",
        "\n",
        "        # Teacher forcing\n",
        "        use_teacher_forcing = (\n",
        "          target_tokens is not None\n",
        "          and torch.rand(1).item() < teacher_forcing_ratio\n",
        "        )\n",
        "\n",
        "        predicted_token = output_logits.argmax(1)\n",
        "\n",
        "        decoder_input_token = (\n",
        "          target_tokens[:, t] if use_teacher_forcing else predicted_token\n",
        "        )\n",
        "\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "LG9K2fC5UhiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM).to(DEVICE)\n",
        "attention = BahdanauAttention(HID_DIM).to(DEVICE)\n",
        "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, attention).to(DEVICE)\n",
        "model = Seq2Seq(encoder, decoder, PAD_IDX).to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "metadata": {
        "id": "e9p-QbhEVIcg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPs2r8EyTkocoPWVWivcns+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}